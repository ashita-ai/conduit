# Conduit Glossary

**Last Updated**: 2025-11-29

Technical terms and concepts used throughout Conduit.

## Core Concepts

### Conduit
The ML-powered LLM routing system. Routes queries to the optimal model based on learned patterns, reducing costs 30-50% while maintaining quality.

### Routing
The process of selecting which LLM model should handle a given query. Conduit uses bandit algorithms to learn optimal routing decisions over time.

### Model Arm
A candidate model that can be selected for routing. Each arm tracks statistics like pull count, mean reward, and cost. Named after the "arms" of a multi-armed bandit.

```python
ModelArm(
    model_id="gpt-4o-mini",
    provider="openai",
    cost_per_input_token=1.1e-6,
    expected_quality=0.85
)
```

---

## Bandit Algorithms

### Multi-Armed Bandit (MAB)
A class of algorithms that balance exploration (trying new options) vs exploitation (using known-good options). Named after slot machines ("one-armed bandits").

### UCB1 (Upper Confidence Bound)
Non-contextual bandit algorithm. Selects arms based on:
- Mean observed reward
- Uncertainty bonus (explores less-tried arms)

Formula: `UCB = mean_reward + c * sqrt(log(total_pulls) / arm_pulls)`

**Used in**: Phase 1 of hybrid routing (cold start)

### LinUCB (Linear UCB)
Contextual bandit algorithm. Uses query features to make context-aware decisions. Maintains per-arm matrices for ridge regression.

Formula: `UCB = theta^T @ x + alpha * sqrt(x^T @ A_inv @ x)`

Where:
- `theta`: Learned coefficients (A_inv @ b)
- `x`: Feature vector (386 dimensions)
- `A`: Covariance matrix
- `b`: Reward-weighted feature sum

**Used in**: Phase 2 of hybrid routing (after warm-up)

### Thompson Sampling
Bayesian bandit algorithm. Samples from posterior distribution of each arm's reward, then selects highest sample. Good exploration/exploitation balance.

### Epsilon-Greedy
Simple bandit algorithm. Explores randomly with probability epsilon, otherwise exploits best-known arm. Baseline for comparison.

### Contextual Thompson Sampling
Bayesian contextual bandit. Combines Thompson Sampling's probabilistic selection with context features.

---

## Feature Extraction

### Query Features
Structured representation of a query for routing decisions:

```python
QueryFeatures(
    embedding=[...],       # 384-dim semantic vector
    token_count=150,       # Estimated tokens
    complexity_score=0.7,  # 0.0-1.0 complexity
    query_text="..."       # Original query for context detection
)
```

### Embedding
Dense vector representation of query semantics. Generated by embedding models (OpenAI, FastEmbed, etc.). Captures meaning for similarity comparison.

| Provider | Dimensions | Notes |
|----------|------------|-------|
| FastEmbed | 384 | Free, local, default |
| OpenAI | 1536 | API-based, higher quality |
| Cohere | 1024 | API-based alternative |

### Feature Vector
Combined representation for LinUCB: embedding + metadata.

```
[384 embedding dims] + [token_count/1000] + [complexity_score] = 386 dims
```

### PCA (Principal Component Analysis)
Dimensionality reduction technique. Compresses embeddings (384 or 1536 dims) to fewer dimensions (typically 64) while preserving most variance.

**Benefits**:
- Faster LinUCB matrix operations
- Reduced memory usage
- Faster convergence (fewer parameters to learn)

**Trade-off**: Slight information loss (typically retains 95%+ variance)

---

## Routing Phases

### Cold Start
Initial period when the system has no learned preferences. Thompson Sampling (default) handles cold start efficiently via Bayesian exploration.

### Default Routing (Thompson Sampling)
The default algorithm as of PR #169. Simple, effective Bayesian bandit:
- No embedding computation needed
- Excellent cold-start performance
- Uses Beta distributions for uncertainty

### Hybrid Routing (Optional)
Two-phase strategy for contextual learning (use `algorithm="hybrid_thompson_linucb"`):

1. **Phase 1 (Thompson Sampling)**: First ~2,000 queries
   - Non-contextual (ignores query content)
   - Bayesian exploration of model quality
   - No embedding computation needed

2. **Phase 2 (LinUCB)**: After threshold
   - Contextual (uses query features)
   - Smart routing based on query type
   - Requires embedding computation

### Transition
Switch from Thompson Sampling to LinUCB at configured threshold. Transfers learned reward estimates to give LinUCB a "warm start."

### Convergence
Point where the bandit has learned stable preferences. Exploration decreases, exploitation increases. Typically 5,000-35,000 queries depending on model count and query diversity.

---

## Feedback & Learning

### Reward
Numerical signal indicating routing quality. Computed from multiple factors:

```python
reward = (
    quality_weight * quality_score +      # 0.7 default
    cost_weight * (1 - normalized_cost) + # 0.2 default
    latency_weight * (1 - normalized_latency)  # 0.1 default
)
```

### Explicit Feedback
Direct quality signals:
- `quality_score`: 0.0-1.0 rating
- `user_rating`: Thumbs up/down
- `met_expectations`: Boolean

### Implicit Feedback
Indirect quality signals (weighted 30% vs explicit 70%):
- Error rates
- Retry attempts
- Response latency
- Token usage patterns

### Bandit Update
Process of learning from feedback:

```python
await bandit.update(
    feedback=BanditFeedback(model_id="gpt-4o", quality_score=0.9, cost=0.002),
    features=query_features
)
```

---

## Evaluation

### Arbiter
LLM-as-judge evaluation system. Uses a cheap/fast model (e.g., o4-mini) to evaluate response quality asynchronously.

**Flow**:
1. Query routed to model
2. Response generated
3. Arbiter evaluates quality (background, non-blocking)
4. Score fed back to bandit

**Configuration**:
- `sample_rate`: Fraction of queries to evaluate (default: 0.1)
- `daily_budget`: Cost limit for evaluation
- `model`: Model for judging (default: o4-mini)

### Quality Score
0.0-1.0 rating of response quality. Can come from:
- Arbiter evaluation
- User feedback
- Heuristics (response length, format compliance)

---

## State & Persistence

### State Store
Persistence layer for bandit state. Survives server restarts.

```python
await router.save_state(store, "production-router")
# Later...
await router.load_state(store, "production-router")
```

### Bandit State
Serializable snapshot of algorithm state:
- UCB1: Pull counts, reward sums per arm
- LinUCB: A matrices, b vectors per arm

### Auto-Persistence
Automatic state saving:
- After every `update()` call (~1-5ms overhead)
- Periodic checkpoints (every N queries)
- On graceful shutdown

---

## Configuration

### conduit.yaml
Primary configuration file:

```yaml
models:
  - gpt-4o-mini
  - gpt-4o
  - claude-3-5-sonnet

routing:
  algorithm: hybrid
  switch_threshold: 2000

arbiter:
  enabled: true
  sample_rate: 0.1
  model: o4-mini
```

### Priors
Initial quality estimates for models (Thompson Sampling):

```yaml
priors:
  gpt-4o:
    alpha: 85  # Successes + 1
    beta: 15   # Failures + 1
```

### Reward Weights
Multi-objective optimization weights:

```yaml
reward_weights:
  quality: 0.7   # Maximize quality (default)
  cost: 0.2      # Minimize cost (default)
  latency: 0.1   # Minimize latency (default)
```

---

## Performance

### Exploration vs Exploitation
Fundamental trade-off in bandit algorithms:
- **Exploration**: Try less-known options to learn
- **Exploitation**: Use best-known option for immediate reward

### Regret
Cumulative difference between optimal selection and actual selection. Lower is better. Bandits minimize regret over time.

### Non-Stationarity
When model quality changes over time (e.g., provider updates). Handled via sliding window that discounts old observations.

```python
LinUCBBandit(arms, window_size=1000)  # Only use last 1000 observations
```

---

## Integration

### LiteLLM
Unified LLM API wrapper. Conduit integrates via callbacks for automatic feedback collection.

```python
from conduit.litellm import ConduitStrategy
litellm.set_callbacks([ConduitStrategy(router)])
```

### PydanticAI
Structured output framework. Used internally for type-safe LLM interactions.

### Provider
LLM service (OpenAI, Anthropic, Google, etc.). Each model belongs to a provider.
