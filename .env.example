# LLM Provider API Keys (all providers supported by PydanticAI)
# Conduit dynamically detects which providers you have keys for

# OpenAI (gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo)
OPENAI_API_KEY=your_openai_key_here

# Anthropic (claude-3-5-sonnet, claude-3-opus, claude-3-haiku)
ANTHROPIC_API_KEY=your_anthropic_key_here

# Google/Gemini (gemini-1.5-pro, gemini-1.5-flash, gemini-1.0-pro)
# NOTE: Use GOOGLE_API_KEY (Conduit normalizes this for PydanticAI)
GOOGLE_API_KEY=your_google_key_here

# Groq (llama-3.1-70b, llama-3.1-8b, mixtral-8x7b)
GROQ_API_KEY=your_groq_key_here

# Mistral (mistral-large, mistral-medium, mistral-small)
MISTRAL_API_KEY=your_mistral_key_here

# Cohere (command-r-plus, command-r)
COHERE_API_KEY=your_cohere_key_here

# AWS Bedrock (claude, llama, titan models via AWS)
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_REGION=us-east-1

# HuggingFace (hosted inference API)
HUGGINGFACE_API_KEY=your_huggingface_key_here

# Database (PostgreSQL - any provider)
# Works with: Self-hosted, AWS RDS, Google Cloud SQL, Supabase, Neon, Railway, etc.
# Format: postgresql://user:password@host:port/database
DATABASE_URL=postgresql://postgres:your_password@localhost:5432/conduit

# Examples for different providers:
# - Local: postgresql://postgres:password@localhost:5432/conduit
# - Supabase: postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres
# - AWS RDS: postgresql://username:password@your-instance.region.rds.amazonaws.com:5432/conduit
# - Neon: postgresql://user:password@ep-xxxxx.region.aws.neon.tech/conduit?sslmode=require

# Redis (optional - for caching)
# Local: redis://localhost:6379
# Redis Cloud: redis://<username>:<password>@<host>:<port>
REDIS_URL=redis://default:your_redis_password@your-redis-host:port
REDIS_USERNAME=default
REDIS_PASSWORD=your_redis_password
REDIS_API_KEY=your_redis_api_key_here

# Application
LOG_LEVEL=INFO
ENVIRONMENT=development

# Feature Dimension Reduction (PCA)
# Enable PCA to reduce embeddings from 384→64 dims (75% fewer samples needed)
# Run examples/04_pca/pca_setup.py first to train PCA model
USE_PCA=false
PCA_DIMENSIONS=64
PCA_MODEL_PATH=models/pca.pkl

# Bandit Algorithm Configuration
# Reward weights (must sum to 1.0)
REWARD_WEIGHT_QUALITY=0.5
REWARD_WEIGHT_COST=0.3
REWARD_WEIGHT_LATENCY=0.2

# Sliding window for non-stationarity (0 = unlimited history)
BANDIT_WINDOW_SIZE=1000

# Success threshold for statistics (0.0-1.0)
BANDIT_SUCCESS_THRESHOLD=0.85

# Exploration rate for epsilon-greedy (0.0-1.0)
EXPLORATION_RATE=0.1

# Hybrid Routing Configuration
# Enable hybrid routing (UCB1→LinUCB warm start) for 30% faster convergence
USE_HYBRID_ROUTING=false
HYBRID_SWITCH_THRESHOLD=2000
HYBRID_UCB1_C=1.5
HYBRID_LINUCB_ALPHA=1.0

# Redis Cache Configuration
REDIS_CACHE_ENABLED=true
REDIS_CACHE_TTL=86400
REDIS_MAX_RETRIES=3
REDIS_TIMEOUT=5
REDIS_CIRCUIT_BREAKER_THRESHOLD=5
REDIS_CIRCUIT_BREAKER_TIMEOUT=300

# Database Configuration
DATABASE_POOL_SIZE=20

# LLM Execution Timeouts (seconds)
LLM_TIMEOUT_DEFAULT=60.0
LLM_TIMEOUT_FAST=30.0
LLM_TIMEOUT_PREMIUM=90.0

# API Server Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RATE_LIMIT=100

# API Security
API_KEY=your_secret_api_key_here
API_REQUIRE_AUTH=false
API_MAX_REQUEST_SIZE=10000

# OpenTelemetry (optional - for observability)
OTEL_ENABLED=false
OTEL_SERVICE_NAME=conduit-router
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_HEADERS=
OTEL_TRACES_ENABLED=true
OTEL_METRICS_ENABLED=true

# Embedding Provider Configuration
# Default: huggingface (free, no API key needed)
# Options: huggingface, openai, cohere, sentence-transformers
EMBEDDING_PROVIDER=huggingface
EMBEDDING_MODEL=
EMBEDDING_API_KEY=

# Note: For OpenAI embeddings, reuse OPENAI_API_KEY above
# For Cohere embeddings, use COHERE_API_KEY above
