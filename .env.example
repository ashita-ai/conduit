# Conduit Environment Variables
# Copy to .env and fill in your values

# =============================================================================
# LLM Provider API Keys (set at least one)
# =============================================================================
# Conduit auto-detects available providers based on which keys are set

# OpenAI (gpt-4o, gpt-4o-mini, o1, o1-mini)
OPENAI_API_KEY=

# Anthropic (claude-sonnet-4, claude-opus-4, claude-haiku-4)
ANTHROPIC_API_KEY=

# Google/Gemini (gemini-2.0-flash, gemini-2.5-pro, gemini-3-pro-preview)
GOOGLE_API_KEY=

# Groq (llama-3.1-70b, llama-3.1-8b, mixtral-8x7b)
GROQ_API_KEY=

# Mistral (mistral-large, mistral-medium, mistral-small)
MISTRAL_API_KEY=

# Cohere (command-r-plus, command-r)
COHERE_API_KEY=

# AWS Bedrock (claude, llama, titan models via AWS)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1

# HuggingFace (hosted inference API, also used for embeddings)
HUGGINGFACE_API_KEY=
HF_TOKEN=

# =============================================================================
# Database (PostgreSQL)
# =============================================================================
# Works with: Self-hosted, AWS RDS, Google Cloud SQL, Supabase, Neon, etc.
# Format: postgresql://user:password@host:port/database
DATABASE_URL=postgresql://postgres:password@localhost:5432/conduit
DATABASE_POOL_SIZE=20

# =============================================================================
# Redis Cache (optional)
# =============================================================================
# Redis is optional. Conduit gracefully degrades without it.
# Format: redis://user:password@host:port
REDIS_URL=redis://localhost:6379
REDIS_CACHE_ENABLED=true
REDIS_CACHE_TTL=86400
REDIS_MAX_RETRIES=3
REDIS_TIMEOUT=5
REDIS_CIRCUIT_BREAKER_THRESHOLD=5
REDIS_CIRCUIT_BREAKER_TIMEOUT=300

# =============================================================================
# Embedding Provider
# =============================================================================
# Default: "auto" - auto-detects in priority order:
#   1. OpenAI (if OPENAI_API_KEY set) - recommended, reuses LLM key
#   2. Cohere (if COHERE_API_KEY set)
#   3. FastEmbed (if installed: pip install fastembed)
#   4. sentence-transformers (if installed)
#
# Options: auto, openai, cohere, fastembed, sentence-transformers, huggingface
EMBEDDING_PROVIDER=auto
EMBEDDING_MODEL=

# PCA Dimensionality Reduction (optional, improves LinUCB convergence)
USE_PCA=false
PCA_COMPONENTS=128

# =============================================================================
# Bandit Algorithm Configuration
# =============================================================================
# Reward weights (must sum to 1.0)
REWARD_WEIGHT_QUALITY=0.5
REWARD_WEIGHT_COST=0.3
REWARD_WEIGHT_LATENCY=0.2

# Exploration rate for epsilon-greedy (0.0-1.0)
EXPLORATION_RATE=0.1

# Sliding window for non-stationarity (0 = unlimited history)
BANDIT_WINDOW_SIZE=1000

# Success threshold for statistics (0.0-1.0)
BANDIT_SUCCESS_THRESHOLD=0.85

# =============================================================================
# Hybrid Routing (UCB1 -> LinUCB warm start)
# =============================================================================
USE_HYBRID_ROUTING=false
HYBRID_SWITCH_THRESHOLD=2000
HYBRID_UCB1_C=1.5
HYBRID_LINUCB_ALPHA=1.0

# =============================================================================
# API Server
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_RATE_LIMIT=100
API_KEY=
API_REQUIRE_AUTH=false
API_MAX_REQUEST_SIZE=10000

# =============================================================================
# LLM Execution Timeouts (seconds)
# =============================================================================
LLM_TIMEOUT_DEFAULT=60.0
LLM_TIMEOUT_FAST=30.0
LLM_TIMEOUT_PREMIUM=90.0

# =============================================================================
# Logging
# =============================================================================
LOG_LEVEL=INFO
ENVIRONMENT=development

# =============================================================================
# Arbiter LLM-as-Judge (optional)
# =============================================================================
ARBITER_ENABLED=false
ARBITER_SAMPLE_RATE=0.1
ARBITER_DAILY_BUDGET=10.0
ARBITER_MODEL=gpt-4o-mini

# =============================================================================
# OpenTelemetry (optional)
# =============================================================================
OTEL_ENABLED=false
OTEL_SERVICE_NAME=conduit-router
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_HEADERS=
OTEL_TRACES_ENABLED=true
OTEL_METRICS_ENABLED=true
