# Conduit Configuration
# See docs/configuration.md for detailed documentation
# Last updated: 2025-11-25

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Default models for routing. Update these when new model versions release.
# Check AGENTS.md "Model Reference" section for current best models.
#
# Sources (Nov 2025):
# - OpenAI GPT-5.1: https://openai.com/index/gpt-5-1-for-developers/
# - Anthropic Claude Opus 4.5: https://www.anthropic.com/news/claude-opus-4-5
# - Google Gemini 3: https://blog.google/products/gemini/gemini-3/

models:
  # Default routing pool (used when no specific models requested)
  # Order matters: first model is primary fallback
  default:
    - o4-mini                    # OpenAI fast reasoning (cost-effective)
    - gpt-5.1                    # OpenAI flagship (76.3% SWE-bench)
    - claude-sonnet-4-5-20241124 # Anthropic balanced
    - gemini-2.5-flash           # Google fast

  # Model for Arbiter LLM-as-judge evaluation (cheap, fast preferred)
  arbiter: o4-mini

  # Global fallback when requested model is unknown
  fallback: o4-mini

# Provider-specific fallbacks for unknown model IDs
# When we see a model ID we don't recognize, use these based on provider pattern
provider_fallbacks:
  openai: o4-mini
  anthropic: claude-haiku-4-5-20241124
  google: gemini-2.5-flash
  meta: llama-4-scout
  mistral: mistral-small-latest
  default: o4-mini               # When provider can't be determined

# =============================================================================
# FALLBACK PRICING (per 1M tokens, USD)
# =============================================================================
# Used when database pricing unavailable. Update monthly from provider sites.
# Source: https://artificialanalysis.ai/leaderboards/providers

pricing:
  # OpenAI models (Nov 2025)
  o4-mini:
    input: 1.10
    output: 4.40
  gpt-5.1:
    input: 2.00
    output: 8.00
  gpt-5.1-codex-mini:
    input: 0.50
    output: 2.00
  # Legacy OpenAI (still supported)
  gpt-4o-mini:
    input: 0.15
    output: 0.60
  gpt-4o:
    input: 2.50
    output: 10.00

  # Anthropic models (Nov 2025 - Claude 4.5 series)
  claude-opus-4-5-20241124:
    input: 5.00
    output: 25.00
  claude-sonnet-4-5-20241124:
    input: 3.00
    output: 15.00
  claude-haiku-4-5-20241124:
    input: 0.80
    output: 4.00

  # Google models (Nov 2025 - Gemini 3 series)
  gemini-3.0-pro:
    input: 1.25
    output: 5.00
  gemini-2.5-flash:
    input: 0.075
    output: 0.30
  gemini-2.5-pro:
    input: 1.25
    output: 5.00

  # Meta models (via Groq/Together)
  llama-4-maverick:
    input: 0.50
    output: 0.50
  llama-4-scout:
    input: 0.11
    output: 0.11

  # Mistral models
  mistral-large-latest:
    input: 2.00
    output: 6.00
  mistral-small-latest:
    input: 0.20
    output: 0.60

  # Conservative default for unknown models
  _default:
    input: 1.00
    output: 3.00

# =============================================================================
# ROUTING CONFIGURATION
# =============================================================================

routing:
  # Default optimization strategy
  # Options: balanced, quality, cost, speed
  default_optimization: balanced

  # Reward weight presets for routing decisions
  # Each preset balances quality, cost, and latency differently
  # Weights must sum to 1.0
  presets:
    balanced:
      quality: 0.7    # Prioritize quality
      cost: 0.2       # Moderate cost concern
      latency: 0.1    # Low latency concern

    quality:
      quality: 0.8    # Maximize quality
      cost: 0.1       # Minimal cost concern
      latency: 0.1    # Minimal latency concern

    cost:
      quality: 0.4    # Acceptable quality
      cost: 0.5       # Minimize cost
      latency: 0.1    # Low latency concern

    speed:
      quality: 0.4    # Acceptable quality
      cost: 0.1       # Low cost concern
      latency: 0.5    # Minimize latency

# Industry-wide priors for cold start optimization
#
# Data Sources (see docs/PRIORS.md for full methodology):
# - Artificial Analysis: https://artificialanalysis.ai/leaderboards/providers
# - Vellum LLM Leaderboard: https://www.vellum.ai/llm-leaderboard
#
# Format: model_id: [alpha, beta] where Beta(alpha, beta) represents prior belief
# Quality estimate = alpha / (alpha + beta)
# Prior strength = alpha + beta (10000 = high confidence)
#
# To update priors from latest benchmarks:
#   python scripts/sync_priors.py --config conduit.yaml
#
priors:
  # Code Context: Code generation, debugging, technical queries
  # Primary benchmark: SWE-Bench (Vellum), LiveCodeBench (Artificial Analysis)
  code:
    gpt-4o: [8000, 2000]              # 80% - Strong coding, based on LiveCodeBench
    gpt-4o-mini: [7200, 2800]         # 72% - Good for simple code tasks
    claude-3-5-sonnet-20241022: [8200, 1800]  # 82% - SWE-Bench 82% (Vellum)
    claude-3-opus-20240229: [8090, 1910]      # 80.9% - SWE-Bench 80.9% (Vellum)

  # Creative Context: Creative writing, storytelling, content generation
  # Primary benchmark: Qualitative assessments, community consensus
  # Note: No standardized creative benchmark; estimates based on model capabilities
  creative:
    gpt-4o: [7800, 2200]              # 78% - Good creative output
    gpt-4o-mini: [6500, 3500]         # 65% - Acceptable for drafts
    claude-3-5-sonnet-20241022: [8800, 1200]  # 88% - Known for nuanced writing
    claude-3-opus-20240229: [9200, 800]       # 92% - Best creative quality

  # Analysis Context: Analytical reasoning, comparison, evaluation
  # Primary benchmark: GPQA Diamond (Vellum), Intelligence Index (AA)
  analysis:
    gpt-4o: [7500, 2500]              # 75% - GPQA ~75% (Vellum)
    gpt-4o-mini: [6000, 4000]         # 60% - Limited reasoning depth
    claude-3-5-sonnet-20241022: [7800, 2200]  # 78% - Strong reasoning
    claude-3-opus-20240229: [8700, 1300]      # 87% - GPQA 87% (Vellum)

  # Simple QA Context: Factual questions, straightforward queries
  # Primary benchmark: MMLU Pro (Artificial Analysis)
  simple_qa:
    gpt-4o: [8200, 1800]              # 82% - MMLU Pro ~82% (AA)
    gpt-4o-mini: [6800, 3200]         # 68% - MMLU Pro ~68% (AA)
    claude-3-5-sonnet-20241022: [8750, 1250]  # 87.5% - MMLU Pro 87.5% (AA)
    claude-3-opus-20240229: [8950, 1050]      # 89.5% - MMLU Pro 89.5% (AA)

  # General Context: Default for unclassified queries
  # Weighted average across benchmarks (see docs/PRIORS.md for weights)
  general:
    gpt-4o: [7900, 2100]              # 79% - Balanced performance
    gpt-4o-mini: [6600, 3400]         # 66% - Cost-effective baseline
    claude-3-5-sonnet-20241022: [8300, 1700]  # 83% - Strong all-around
    claude-3-opus-20240229: [8700, 1300]      # 87% - Premium quality
